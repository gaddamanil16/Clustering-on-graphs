{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda, Input, Dense\n",
    "from keras.models import Model\n",
    "#from keras.datasets import mnist\n",
    "#from keras.losses import mse, binary_crossentropy\n",
    "#from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#import argparse\n",
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = \"device=cuda,exception_verbosity=high\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "#IN_FILE = '/home/anil/Project/Ericsson/location/DPA_cellid_date_20170126.csv'\n",
    "IN_FILE = 'DPA_cellid_date_20170126.csv'\n",
    "\n",
    "#read data file\n",
    "df = pd.read_csv(IN_FILE,header = 0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#process the list of BTS for each user\n",
    "df_ = df['cellid_set_count_1d_MOD'].str.split(',')\n",
    "def get_bs(x):\n",
    "    bs = []\n",
    "    num = []\n",
    "    try:\n",
    "        for i in x:\n",
    "            try:\n",
    "                i_ = i.split(':')\n",
    "                if (i_[0] != ''):\n",
    "                    if (np.int(i_[1]) > 0):\n",
    "                        bs.append(i_[0])\n",
    "                        num.append(np.int(i_[1]))\n",
    "            except:\n",
    "                0\n",
    "    except:\n",
    "        0\n",
    "    return bs\n",
    "df_ = pd.DataFrame(df_)\n",
    "#the column 'bss' contain the list of BTS for each user \n",
    "df_['bss'] = df_['cellid_set_count_1d_MOD'].apply(get_bs)\n",
    "#filter out users with no BTSs\n",
    "df_ = df_[df_.bss.apply(len) != 0]\n",
    "\n",
    "df_user26 = df_.copy()\n",
    "\n",
    "df_user26['bssCount'] = [len(c) for c in df_user26['bss']]\n",
    "\n",
    "filter_users = list(df_user26[df_user26['bssCount']>=100].index)\n",
    "\n",
    "df_user26_filtered = df_user26.loc[filter_users]\n",
    "\n",
    "df_user26_filtered['bss_str'] = df_user26_filtered.bss.apply(lambda word_list : ' '.join(word_list))\n",
    "\n",
    "#list of documents for each day\n",
    "data26 = df_user26_filtered.loc[:,'bss_str'].tolist()\n",
    "\n",
    "min_users = 100 #minimum users per bts\n",
    "\n",
    "#vectorize the sing data to get the BSs that satisfy the minium user condition\n",
    "tf_vectorizer26 = CountVectorizer(max_df=1.0, min_df=min_users, lowercase = False)\n",
    "tf26 = tf_vectorizer26.fit_transform(data26) #Gives a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U,B = tf26.shape\n",
    "\n",
    "observed = tf26.T\n",
    "# we can split observed into training and testing for prediction step\n",
    "x_train = observed\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "    # Arguments:\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns:\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (U, )\n",
    "batch_size = 512\n",
    "latent_dim = 250\n",
    "epochs = 50\n",
    "encoding_dim = int(U/20)  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = Dense(encoding_dim, activation='relu')(inputs)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "#plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
    "#%%\n",
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(encoding_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(U, activation='sigmoid')(x)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "#plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae_mlp')\n",
    "\n",
    "def differences(pair_of_tensors):\n",
    "    x, y = pair_of_tensors\n",
    "    return K.abs(x - y)\n",
    "\n",
    "def ham_loss(y_true,y_pred):\n",
    "    # calculate pairwise hamming distance matrix\n",
    "    # differences of y_pred \n",
    "    pairwise_diff_pred = K.abs(K.expand_dims(y_pred, 0) - K.expand_dims(y_pred, 1))\n",
    "    pairwise_distance_pred = K.sum(pairwise_diff_pred, axis=-1)\n",
    "    \n",
    "    # calculate pairwise hamming distance matrix for inputs\n",
    "    pairwise_diff_true = K.abs(K.expand_dims(y_true, 0) - K.expand_dims(y_true, 1))\n",
    "    pairwise_distance_true = K.sum(pairwise_diff_true, axis=-1)\n",
    "    \n",
    "    #Difference between the distances of y_true and y_predictions\n",
    "    hamm_sum= Lambda(differences)([pairwise_distance_true, pairwise_distance_pred])\n",
    "    \n",
    "    return K.sum(hamm_sum, axis=-1)\n",
    "    \n",
    "def vae_loss(y_true, y_pred):\n",
    "    \"\"\" Calculate loss = reconstruction loss + KL loss for each data in minibatch \"\"\"\n",
    "    # E[log P(X|z)]\n",
    "    #recon = K.sum(mean_squared_error(y_true,y_pred))\n",
    "    recon = K.sum(K.binary_crossentropy(y_true,y_pred),axis=1)\n",
    "    # D_KL(Q(z|X) || P(z|X)); calculate in closed form as both dist. are Gaussian\n",
    "    kl = 0.5 * K.sum(K.exp(z_log_var) + K.square(z_mean) - 1. - z_log_var, axis=1)\n",
    "    \n",
    "    hamming_loss = ham_loss(y_true,y_pred)\n",
    "\n",
    "    return recon + kl + hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.compile(optimizer='adam', loss=vae_loss)\n",
    "vae.fit(x_train, x_train, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_bts,_,_ = encoder.predict(x_train)\n",
    "#print(encoded_bts.shape)\n",
    "K_cluster = 100\n",
    "\n",
    "##Fit GMM to the encoded graph\n",
    "from sklearn import mixture\n",
    "gmm = mixture.GaussianMixture(n_components= K_cluster, covariance_type='full')\n",
    "y = gmm.fit(encoded_bts)\n",
    "\n",
    "a_ = y.predict(encoded_bts)\n",
    "\n",
    "gmm_clus = pd.DataFrame(a_, index = tf_vectorizer26.get_feature_names())\n",
    "gmm_clus.to_csv('/raid60/anil.gaddam/vae_hamloss.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
